
<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <meta name="keywords" content="eccv, workshop, computer vision, natural language processing, computer graphics, visual learning, machine learning">

  <!-- <link rel="shortcut icon" href="static/img/site/favicon.png"> -->

  <title>World Model Bench @ CoRL'25</title>
  <meta name="description" content="World Model Bench, CoRL 2025 Workshop">

  <!--Open Graph Related Stuff-->
  <meta property="og:title" content="World Model Bench Workshop"/>
  <meta property="og:url" content="https://worldmodelbench2.github.io"/>
  <meta property="og:description" content="World Model Bench, CoRL 2025 Workshop"/>
  <meta property="og:site_name" content="World Model Bench Workshop"/>
  <!-- <meta property="og:image" content="https://languagefor3dscenes.github.io/ICCV2023/static/img/site/teaser.jpg"/> -->

  <!--Twitter Card Stuff-->
  <meta name="twitter:card" content="summary_large_image"/>
  <meta name="twitter:title" content="World Model Bench Workshop"/>
  <!-- <meta name="twitter:image" content="https://languagefor3dscenes.github.io/ICCV2023/static/img/site/teaser.jpg"> -->
  <meta name="twitter:url" content="https://worldmodelbench2.github.io"/>
  <meta name="twitter:description" content="World Model Bench, CoRL 2025 Workshop"/>

  <!-- CSS  -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
  <link rel="stylesheet" href="static/css/main.css" media="screen,projection">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

  <style>

    .people-pic {
      max-width: 125px;
      max-height: 125px;
      /*width:300px;*/
      /*height:300px;*/
      object-fit: cover;
      border-radius: 50%;
  }
  </style>
</head>

  <body>

    <!-- <div class="top-strip"></div> -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="container">
    
    <div class="navbar-header">
      <a class="navbar-brand" href="/"></a>
      <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>

    <div class="navbar-collapse collapse" id="navbar-main">
      <ul class="nav navbar-nav">
        <li><a href="#intro">Introduction</a></li>
        <li><a href="#cfp">Call for papers</a></li>
        <li><a href="#dates">Schedule</a></li>
        <li><a href="#speakers">Invited Speakers</a></li>
        <li><a href="#organizers">Organizers</a></li>
        <li><a href="#contact">Contact</a></li>
        <!-- <li class="dropdown">
          <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Past Workshops <span class="caret"></span></a>
          <ul class="dropdown-menu">
            <li><a href="../CVPR2021/index.html" target="__blank">CVPR 2021</a></li>
            <li><a href="../ECCV2022/" target="__blank">ECCV 2022</a></li>
          </ul>
        </li> -->
      </ul>
    </div>

  </div>
</div>


    <div class="container">
      <div class="page-content">
          <p><br /></p>
<div class="row">
  <div class="col-xs-12">
    <center><h1>WorldModelBench: Benchmarking World Models for Robot Learning</h1></center>
    <center><h2>CoRL 2025 Workshop</h2></center>
    <!-- <center>Sep 27, 2025</center> -->
  </div>
</div>

<hr />


<!-- <br>
  <center>
  <h1 style="color:red"><a href="https://www.youtube.com/watch?v=gyJDGrbLknI">The <b>video recording</b> of this workshop is here!</a></h1>
  </center>
<br> -->

<!-- <div class="alert alert-info" role="alert">
  <b>Join Zoom Meeting  <a href="">here</a>.</b>
</div> -->



<!-- <div class="row" id="teaser">  
    <div>  
    <img src="static/img/site/teaser.jpg" style="width: 100%; height: auto;"/>
  </div>
</div> -->


<!-- <div class="col-xs-6 col-sm-6 col-md-6 col-lg-6"> 
  <img src="static/img/site/a.png">
</div>

<div class="col-xs-6 col-sm-6 col-md-6 col-lg-6"> 
  <img src="static/img/site/b.png">
</div>
 -->






<p><br /></p>
<div class="row" id="intro">
  <div class="col-xs-12">
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      The rapid emergence of video-based world models has opened exciting new frontiers in robot learning, where such models are increasingly used as predictive simulators for perception, planning, and control. These world models aim to capture the physics and dynamics of real-world environments, enabling embodied agents to reason about the future, perform counterfactual simulations, and learn from visual experience. Despite recent breakthroughs in model design and scalability, evaluating the capabilities of these world models remains an open challenge. Recent benchmarks such as WorldScore, WorldSimBench, VideoPhy, Physics-IQ, and VBench have made important strides in evaluating world models across key dimensions, including controllability, dynamics, physical plausibility, and alignment with prompts and goals. These efforts reveal that while modern models can generate visually compelling videos, many still struggle with physical commonsense, temporal coherence, or robot-action consistency, which are crucial for downstream robotic applications. 
    </p>
    <p>
      This workshop at CoRL 2025 seeks to bring together the community to explore how world models can be systematically evaluated and improved to serve as robust backbones for robotic perception, planning, and interaction. The workshop will facilitate discussions on benchmark design, evaluation protocols, and task-driven validation, with the overarching goal of making world models more actionable and trustworthy for real-world robot learning. Invited presenters and panelists will be drawn from leading groups working on video generation, simulation, video foundation models, with representation from both academia (e.g., Stanford, HKU, NTU) and industry (e.g., Google, Meta, NVIDIA).
    </p>
  </div>
</div>

<p><br /></p>

<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Call For Papers</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
      <p>
        We welcome submissions on any aspects related to evaluating world-models, including but not limited to:
      </p>
      <ul>
        <li>Methods for developing world (and video) models, including novel architectures, training approaches, and scaling strategies</li>
        <li>Applications of world foundation models and video generation models to downstream embodied tasks, such as robotics and autonomous driving</li>
        <li>Novel metrics, benchmarks or datasets to evaluate world models</li>
        <li>Analysis of safety considerations and potential biases in world foundation models and video generation models</li>
      </ul>
      <p>
        <span style="font-weight:500;">Submission Guideline:</span> 
        <ul>
          <li>Submission website: <a href="https://openreview.net/group?id=thecvf.com/CVPR/2025/Workshop/WorldModelBench&referrer=%5BHomepage%5D(%2F)#tab-your-consoles">openreview submission page</a></li>
          <li>Our workshop accepts both full paper submissions (4-8 pages excluding references) and extended abstract submissions (2-4 pages including references).</li>
          <li>Full paper submissions (4-8 pages excluding references) should NOT be published before. Please refer to CVPR 2025 author guidelines: <a href="https://cvpr.thecvf.com/Conferences/2025/AuthorGuidelines">https://cvpr.thecvf.com/Conferences/2025/AuthorGuidelines</a></li>
          <li>Submission Format: <a href="https://github.com/cvpr-org/author-kit/releases">official CVPR template</a> (double column; no more than 8 pages, excluding reference).</li>
          <li>Our paper reviewing process is double blind.</li>
        </ul>
      </p>
  </div>
</div>                                                                                        
<!-- 
<p><br /></p>
<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Accepted Papers</h2>
  </div>
</div>
<div class="row">
  <div class="col-md-12">
    <table>
      <tbody> 
      <tr><td><a href="https://arxiv.org/abs/2212.01558">#1. PartSLIP: Low-Shot Part Segmentation for 3D Point Clouds via Pretrained Image-Language Models</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Minghua Liu, Yinhao Zhu, Hong Cai, Shizhong Han, Zhan Ling, Fatih Porikli, Hao Su</font></td></tr>
      <tr><td><a href="https://arxiv.org/abs/2112.08359">#2. 3D Question Answering</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Shuquan Ye, Dongdong Chen, Songfang Han, Jing Liao</font></td></tr>
      <tr><td><a href="https://arxiv.org/abs/2211.11682">#3. PointCLIP V2: Prompting CLIP and GPT for Powerful 3D Open-world Learning</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Ziyao Zeng, Zipeng Qin, Shanghang Zhang, Peng Gao</font></td> </tr>        
      <tr><td><a href="https://arxiv.org/abs/2211.16312">#4. PLA: Language-Driven Open-Vocabulary 3D Scene Understanding</a> <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang, Song Bai, Xiaojuan Qi</font></td></tr>
      <tr><td><a href="https://arxiv.org/abs/2303.16894">#5. ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Zoey Guo, Yiwen Tang, Ray Zhang, Dong Wang, Zhigang Wang, Bin Zhao, Xuelong Li</font></td></tr>
      <tr><td><a href="https://arxiv.org/abs/2211.15654">#6.  OpenScene: 3D Scene Understanding with Open Vocabularies</a> <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Songyou Peng, Kyle Genova, Chiyu "Max" Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas Funkhouser</font></td></tr>
      <tr><td><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Kurita_RefEgo_Referring_Expression_Comprehension_Dataset_from_First-Person_Perception_of_Ego4D_ICCV_2023_paper.pdf">#7. RefEgo: Referring Expression Comprehension Dataset from First-Person Perception of Ego4D </a> <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Shuhei Kurita, Naoki Katsura, Eri Onami</font></td></tr>
      <tr><td><a href="https://arxiv.org/abs/2303.12236">#8. SALAD: Part-Level Latent Diffusion for 3D Shape Generation and Manipulation</a> <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Juil Koo, Seungwoo Yoo, Minh Hieu Nguyen, Minhyuk Sung</font></td></tr>
      <tr><td><a href="https://3d-vista.github.io">#9. 3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment</a> <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Ziyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan Huang, Qing Li</font></td></tr>
    </tbody></table>
  </div>

</div> -->

<p><br /></p>

<div class="row" id="dates">
  <div class="col-xs-12">
    <h2>Important Dates (Anywhere on Earth)</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <table class="table table-striped">
      <tbody>
        <tr>
          <td>Paper submission deadline</td>
          <td>April 18th, 2025</td>
        </tr>
        <tr>
          <td>Notifications to accepted papers</td>
          <td>May 16th, 2025</td>
        </tr>
        <tr>
          <td>Paper camera ready</td>
          <td>May 23th, 2025</td>
        </tr>      
      </tbody>
    </table>
  </div>
</div>

<p><br /></p>
<div class="row" id="schedule">
  <div class="col-xs-12">
    <h2>Schedule (Tentative)</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <table class="table table-striped">
      <tbody>
        <tr>
          <td>Introduction and Opening Remarks</td>
          <td>TBD</td>
        </tr>
        <tr>
          <td>Spotlight Presentation 1</td>
          <td>TBD</td>
        </tr>
        <tr>
          <td>Spotlight Presentation 2</td>
          <td>TBD</td>
        </tr>
        <tr>
          <td>Spotlight Presentation 3</td>
          <td>TBD</td>
        </tr>
        <tr>
          <td>Coffee Break</td>
          <td>TBD</td>
        </tr>
        <tr>
          <td>Poster Session</td>
          <td>TBD</td>
        </tr>
        <tr>
          <td>Invited Talk 1</td>
          <td>TBD</td>
        </tr>
        <tr>
          <td>Invited Talk 2</td>
          <td>TBD</td>
        </tr>
        <tr>
          <td>Invited Talk 3</td>
          <td>TBD</td>
        </tr>
        <tr>
          <td>Roundtable Discussion</td>
          <td>TBD</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<p><br /></p>
<div class="row" id="speakers">
  <div class="col-xs-12">
    <h2>Invited Speakers</h2>
  </div>
</div>

<!-- <p>TBD</p> -->

<div class="row">
  <div class="col-md-2">
    <a href="https://wenhuchen.github.io/"><img class="people-pic" style="float:left;margin-right:50px;" src="static/img/people/wenhu.jpg" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://wenhuchen.github.io/">Wenhu Chen</a></b> is a Professor at University of Waterloo and Vector Institute, also a Research Scientist at Google Deepmind. His research interest lies in natural language processing, deep learning and multimodal learning. He aims to design models to handle complex reasoning scenarios like math problem-solving, structure knowledge grounding, etc. He received the Area Chair Award in AACL-IJCNLP 2023, the Best Paper Honorable Mention in WACV 2021, and the UCSB CS Outstanding Dissertation Award in 2021.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-2">
    <a href="https://deeptigp.github.io/"><img class="people-pic" style="float:left;margin-right:50px;" src="static/img/people/deepti.png" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://deeptigp.github.io/">Deepti Ghadiyaram</a></b> is a Professor at Boston University and a member of Technical Staff at Runway. Her research focuses on improving the safety, interpretability, and robustness of AI systems. Previously she spent over 5 years at Meta AI Research working on image and video understanding models, fair and inclusive computer vision models, and ML explainability. She has served as a Program Chair for NeurIPS 2022 Dataset and Benchmarks track, hosted several tutorials and organized workshops and an Area Chair for CVPR, ICCV, ECCV, and NeurIPS.
    </p>
  </div>
</div>


<p><br /></p>
<div class="row">
  <div class="col-md-2">
    <a href="https://aditya-grover.github.io/"><img class="people-pic" style="float:left;margin-right:50px;" src="static/img/people/aditya.jpg" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://aditya-grover.github.io/">Aditya Grover</a></b> is a Professor at UCLA and a co-founder of Inception Labs. He leads the Machine Intelligence (MINT) group at UCLA to develop AI systems that can interact and reason with limited supervision. His current research is at the intersection of generative models and sequential decision making. He received many prestigious awards, such as NSF Career Award, Schmidt AI 2050 Early Career Fellowship, Kavli Fellow by the US National Academy of Sciences, Outstanding Paper Award at NeurIPS, etc.
    </p>
  </div>
</div>

<p><br /></p>

<div class="row" id="organizers">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>

<div class="row">

  <div class="col-xs-2">
    <a href="https://hengcv.github.io/">
      <img class="people-pic" src="static/img/people/heng.jpg" />
    </a>
    <div class="people-name">
      <a href="https://hengcv.github.io/">Heng Wang</a>
      <h6>NVIDIA</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://prithv1.xyz/">
      <img class="people-pic" src="static/img/people/prithvijit.jpeg" />
    </a>
    <div class="people-name">
      <a href="https://prithv1.xyz/">Prithvijit Chattopadhyay</a>
      <h6>NVIDIA</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://github.com/mingyuliutw">
      <img class="people-pic" src="static/img/people/mingyu.jpeg" />
    </a>
    <div class="people-name">
      <a href="https://github.com/mingyuliutw">Ming-Yu Liu</a>
      <h6>NVIDIA</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://sites.google.com/view/showlab/home?authuser=0">
      <img class="people-pic" src="static/img/people/mike.png" />
    </a>
    <div class="people-name">
      <a href="https://sites.google.com/view/showlab/home?authuser=0">Mike Zheng Shou</a>
      <h6>National University of Singapore</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="http://zhangjiewu.github.io">
      <img class="people-pic" src="static/img/people/jay.png" />
    </a>
    <div class="people-name">
      <a href="http://zhangjiewu.github.io">Jay Zhangjie Wu</a>
      <h6>National University of Singapore</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://xh-liu.github.io/">
      <img class="people-pic" src="static/img/people/xihui.jpeg" />
    </a>
    <div class="people-name">
      <a href="https://xh-liu.github.io/">Xihui Liu</a>
      <h6>University of Hong Kong</h6><br><br>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://deeptigp.github.io/">
      <img class="people-pic" src="static/img/people/deepti.png" />
    </a>
    <div class="people-name">
      <a href="https://deeptigp.github.io/">Deepti Ghadiyaram</a>
      <h6>Boston University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://somepago.github.io/">
      <img class="people-pic" src="static/img/people/gowthami.jpg" />
    </a>
    <div class="people-name">
      <a href="https://somepago.github.io/">Gowthami Somepalli</a>
      <h6>University of Maryland, College Park</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://www.huaxiuyao.io/">
      <img class="people-pic" src="static/img/people/huaxiu.png" />
    </a>
    <div class="people-name">
      <a href="https://www.huaxiuyao.io/">Huaxiu Yao</a>
      <h6>University of North Carolina at Chapel Hill</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://wenhuchen.github.io/">
      <img class="people-pic" src="static/img/people/wenhu.jpg" />
    </a>
    <div class="people-name">
      <a href="https://wenhuchen.github.io/">Wenhu Chen</a>
      <h6>University of Waterloo</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://tsong.me/">
      <img class="people-pic" src="static/img/people/jiaming.jpeg" />
    </a>
    <div class="people-name">
      <a href="https://tsong.me/">Jiaming Song</a>
      <h6>Luma AI</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://www.humphreyshi.com/">
      <img class="people-pic" src="static/img/people/humphrey.jpg" />
    </a>
    <div class="people-name">
      <a href="https://www.humphreyshi.com/">Humphrey Shi</a>
      <h6>Georgia Tech</h6>
    </div>
  </div>

</div>
<!-- <p><br /></p>
<p><br /></p> -->
<!-- 
<div class="row" id="advisors">
  <div class="col-xs-12">
    <h2>Senior Advisors</h2>
  </div>
</div>

<div class="row">

  <div class="col-xs-2">
    <a href="http://optas.github.io">
      <img class="people-pic" src="static/img/people/achlioptas.jpg" />
    </a>
    <div class="people-name">
      <a href="http://optas.github.io">Panos Achlioptas</a>
      <h6>Steel Perlot</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://angelxuanchang.github.io/">
      <img class="people-pic" src="static/img/people/angel.jpg" />
    </a>
    <div class="people-name">
      <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>
      <h6>Simon Fraser University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://niessnerlab.org/members/matthias_niessner/profile.html">
      <img class="people-pic" src="static/img/people/matthias.jpg" />
    </a>
    <div class="people-name">
      <a href="https://niessnerlab.org/members/matthias_niessner/profile.html">Matthias Niessner</a>
      <h6>Technical University of Munich</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://geometry.stanford.edu/member/guibas/">
      <img class="people-pic" src="static/img/people/guibas.jpg" />
    </a>
    <div class="people-name">
      <a href="https://geometry.stanford.edu/member/guibas/">Leonidas J. Guibas</a>
      <h6>Stanford University</h6>
    </div>
  </div>
</div> -->

<p><br /></p>

<div class="row" id="contact">
  <div class="col-xs-12">
    <h2>Contact</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <!-- <p>
      To contact the organizers please use <b>worldmodelbench@gmail.com</b>
    </p> -->
    <p>
      To contact the organizers please use <b>worldmodelbench@gmail.com</b>
    </p>
  </div>
</div>
<p><br /></p>

<hr />

<div class="row">
  <div class="col-xs-12">
    <h2>Acknowledgments</h2>
  </div>
</div>
<p><a name="/acknowledgements"></a></p>
<div class="row">
  <div class="col-xs-12">
    <p>
      Thanks to <span style="color:#1a1aff;font-weight:400;"> <a href="https://languagefor3dscenes.github.io/">languagefor3dscenes</a></span> for the webpage format.
    </p>
  </div>
</div>

      </div>
    </div>

  </body>
</html>
