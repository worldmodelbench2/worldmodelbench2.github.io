
<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <meta name="keywords" content="eccv, workshop, computer vision, natural language processing, computer graphics, visual learning, machine learning">

  <!-- <link rel="shortcut icon" href="static/img/site/favicon.png"> -->

  <title>World Model Bench @ CoRL'25</title>
  <meta name="description" content="World Model Bench, CoRL 2025 Workshop">

  <!--Open Graph Related Stuff-->
  <meta property="og:title" content="World Model Bench Workshop"/>
  <meta property="og:url" content="https://worldmodelbench2.github.io"/>
  <meta property="og:description" content="World Model Bench, CoRL 2025 Workshop"/>
  <meta property="og:site_name" content="World Model Bench Workshop"/>
  <!-- <meta property="og:image" content="https://languagefor3dscenes.github.io/ICCV2023/static/img/site/teaser.jpg"/> -->

  <!--Twitter Card Stuff-->
  <meta name="twitter:card" content="summary_large_image"/>
  <meta name="twitter:title" content="World Model Bench Workshop"/>
  <!-- <meta name="twitter:image" content="https://languagefor3dscenes.github.io/ICCV2023/static/img/site/teaser.jpg"> -->
  <meta name="twitter:url" content="https://worldmodelbench2.github.io"/>
  <meta name="twitter:description" content="World Model Bench, CoRL 2025 Workshop"/>

  <!-- CSS  -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
  <link rel="stylesheet" href="static/css/main.css" media="screen,projection">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

  <style>

    .people-pic {
      max-width: 125px;
      max-height: 125px;
      /*width:300px;*/
      /*height:300px;*/
      object-fit: cover;
      border-radius: 50%;
  }
  </style>
</head>

  <body>

    <!-- <div class="top-strip"></div> -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="container">
    
    <div class="navbar-header">
      <a class="navbar-brand" href="/"></a>
      <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>

    <div class="navbar-collapse collapse" id="navbar-main">
      <ul class="nav navbar-nav">
        <li><a href="#intro">Introduction</a></li>
        <li><a href="#dates">Format and Schedule</a></li>
        <li><a href="#speakers">Invited Speakers</a></li>
        <li><a href="#organizers">Organizers</a></li>
        <li><a href="#contact">Contact</a></li>
        <!-- <li class="dropdown">
          <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Past Workshops <span class="caret"></span></a>
          <ul class="dropdown-menu">
            <li><a href="../CVPR2021/index.html" target="__blank">CVPR 2021</a></li>
            <li><a href="../ECCV2022/" target="__blank">ECCV 2022</a></li>
          </ul>
        </li> -->
      </ul>
    </div>

  </div>
</div>


    <div class="container">
      <div class="page-content">
          <p><br /></p>
<div class="row">
  <div class="col-xs-12">
    <center><h1>WorldModelBench: Benchmarking World Models for Robot Learning</h1></center>
    <center><h2>CoRL 2025 Workshop</h2></center>
    <!-- <center>Sep 27, 2025</center> -->
  </div>
</div>

<hr />


<!-- <br>
  <center>
  <h1 style="color:red"><a href="https://www.youtube.com/watch?v=gyJDGrbLknI">The <b>video recording</b> of this workshop is here!</a></h1>
  </center>
<br> -->

<!-- <div class="alert alert-info" role="alert">
  <b>Join Zoom Meeting  <a href="">here</a>.</b>
</div> -->



<!-- <div class="row" id="teaser">  
    <div>  
    <img src="static/img/site/teaser.jpg" style="width: 100%; height: auto;"/>
  </div>
</div> -->


<!-- <div class="col-xs-6 col-sm-6 col-md-6 col-lg-6"> 
  <img src="static/img/site/a.png">
</div>

<div class="col-xs-6 col-sm-6 col-md-6 col-lg-6"> 
  <img src="static/img/site/b.png">
</div>
 -->






<p><br /></p>
<div class="row" id="intro">
  <div class="col-xs-12">
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      The rapid emergence of video-based world models has opened exciting new frontiers in robot learning, where such models are increasingly used as predictive simulators for perception, planning, and control. These world models aim to capture the physics and dynamics of real-world environments, enabling embodied agents to reason about the future, perform counterfactual simulations, and learn from visual experience. Despite recent breakthroughs in model design and scalability, evaluating the capabilities of these world models remains an open challenge. Recent benchmarks such as WorldScore, WorldSimBench, VideoPhy, Physics-IQ, and VBench have made important strides in evaluating world models across key dimensions, including controllability, dynamics, physical plausibility, and alignment with prompts and goals. These efforts reveal that while modern models can generate visually compelling videos, many still struggle with physical commonsense, temporal coherence, or robot-action consistency, which are crucial for downstream robotic applications. 
    </p>
    <p>
      This workshop at CoRL 2025 seeks to bring together the community to explore how world models can be systematically evaluated and improved to serve as robust backbones for robotic perception, planning, and interaction. The workshop will facilitate discussions on benchmark design, evaluation protocols, and task-driven validation, with the overarching goal of making world models more actionable and trustworthy for real-world robot learning. Invited presenters and panelists will be drawn from leading groups working on video generation, simulation, video foundation models, with representation from both academia (e.g., Stanford, HKU, NTU) and industry (e.g., Google, Meta, NVIDIA).
    </p>
  </div>
</div>

<p><br /></p>

<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Format and Schedule</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
      <p>
        The workshop will be structured as a half-day event (approximately 4 hours), featuring a mix of invited talks, a panel discussion, and interactive audience engagement. We will begin with a series of invited talks (20-30 minutes each) from leading researchers in world model development, video generation, and robot learning. These talks will highlight recent advances, emerging benchmarks, and use cases in robotics. Following the talks, we will host a moderated panel discussion with a diverse set of experts from both academia and industry to debate key open questions around benchmarking, generalization, physical realism, and deployment of world models in embodied agents. To ensure active participation, we will solicit questions from the community in advance, allowing audience-driven topics to guide part of the panel. Time will also be allocated for open Q&A and discussion.
      </p>
  </div>
</div>                                                                                        
<!-- 
<p><br /></p>
<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Accepted Papers</h2>
  </div>
</div>
<div class="row">
  <div class="col-md-12">
    <table>
      <tbody> 
      <tr><td><a href="https://arxiv.org/abs/2212.01558">#1. PartSLIP: Low-Shot Part Segmentation for 3D Point Clouds via Pretrained Image-Language Models</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Minghua Liu, Yinhao Zhu, Hong Cai, Shizhong Han, Zhan Ling, Fatih Porikli, Hao Su</font></td></tr>
      <tr><td><a href="https://arxiv.org/abs/2112.08359">#2. 3D Question Answering</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Shuquan Ye, Dongdong Chen, Songfang Han, Jing Liao</font></td></tr>
      <tr><td><a href="https://arxiv.org/abs/2211.11682">#3. PointCLIP V2: Prompting CLIP and GPT for Powerful 3D Open-world Learning</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Ziyao Zeng, Zipeng Qin, Shanghang Zhang, Peng Gao</font></td> </tr>        
      <tr><td><a href="https://arxiv.org/abs/2211.16312">#4. PLA: Language-Driven Open-Vocabulary 3D Scene Understanding</a> <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang, Song Bai, Xiaojuan Qi</font></td></tr>
      <tr><td><a href="https://arxiv.org/abs/2303.16894">#5. ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Zoey Guo, Yiwen Tang, Ray Zhang, Dong Wang, Zhigang Wang, Bin Zhao, Xuelong Li</font></td></tr>
      <tr><td><a href="https://arxiv.org/abs/2211.15654">#6.  OpenScene: 3D Scene Understanding with Open Vocabularies</a> <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Songyou Peng, Kyle Genova, Chiyu "Max" Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas Funkhouser</font></td></tr>
      <tr><td><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Kurita_RefEgo_Referring_Expression_Comprehension_Dataset_from_First-Person_Perception_of_Ego4D_ICCV_2023_paper.pdf">#7. RefEgo: Referring Expression Comprehension Dataset from First-Person Perception of Ego4D </a> <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Shuhei Kurita, Naoki Katsura, Eri Onami</font></td></tr>
      <tr><td><a href="https://arxiv.org/abs/2303.12236">#8. SALAD: Part-Level Latent Diffusion for 3D Shape Generation and Manipulation</a> <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Juil Koo, Seungwoo Yoo, Minh Hieu Nguyen, Minhyuk Sung</font></td></tr>
      <tr><td><a href="https://3d-vista.github.io">#9. 3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment</a> <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Ziyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan Huang, Qing Li</font></td></tr>
    </tbody></table>
  </div>

</div> -->

<p><br /></p>
<div class="row" id="speakers">
  <div class="col-xs-12">
    <h2>Invited Speakers</h2>
  </div>
</div>

<!-- <p>TBD</p> -->

<div class="row">
  <div class="col-md-2">
    <a href="https://liuziwei7.github.io/"><img class="people-pic" style="float:left;margin-right:50px;" src="static/img/people/ziwei.png" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://liuziwei7.github.io/">Ziwei Liu</a></b> is an Associate Professor at MMLab@NTU, College of Computing and Data Science in Nanyang Technological University, Singapore.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-2">
    <a href="https://xh-liu.github.io/"><img class="people-pic" style="float:left;margin-right:50px;" src="static/img/people/xihui.jpeg" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://xh-liu.github.io/">Xihui Liu</a></b> is an Assistant Professor at the Department of Electrical and Electronic Engineering and Institute of Data Science (IDS), The University of Hong Kong.
    </p>
  </div>
</div>
<p><br /></p>
        
<div class="row">
  <div class="col-md-2">
    <a href="http://www.zhangruimao.site/"><img class="people-pic" style="float:left;margin-right:50px;" src="static/img/people/ruimao.png" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="http://www.zhangruimao.site/">Ruimao Zhang</a></b> is a Tenure-track Associate Professor in the Spatial Artificial Intelligence Lab with the School of Electronics and Communication Engineering, Sun Yat-sen University.
    </p>
  </div>
</div>
<p><br /></p>
        
<div class="row">
  <div class="col-md-2">
    <a href="https://jiajunwu.com/"><img class="people-pic" style="float:left;margin-right:50px;" src="static/img/people/jiajun.jpg" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://jiajunwu.com/">Jiajun Wu</a></b> is an Assistant Professor of Computer Science and, by courtesy, of Psychology at Stanford University.
    </p>
  </div>
</div>
<p><br /></p>
        
<div class="row">
  <div class="col-md-2">
    <a href="https://robertgeirhos.com/"><img class="people-pic" style="float:left;margin-right:50px;" src="static/img/people/robert.webp" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://robertgeirhos.com/">Robert Geirhos</a></b> is a Senior Research Scientist at Google DeepMind (formerly Google Brain), located in Toronto. 
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-2">
    <a href="https://www.gujinwei.org/"><img class="people-pic" style="float:left;margin-right:50px;" src="static/img/people/jinwei.jpg" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://www.gujinwei.org/">Jinwei Gu</a></b> is a principal research scientist at NVIDIA, working on deep generative models, vision foundation models, world models, and the general fields of computer vision, computer graphics, and machine learning.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-2">
    <a href="https://www.nicklashansen.com/"><img class="people-pic" style="float:left;margin-right:50px;" src="static/img/people/nicklas.jpeg" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://www.nicklashansen.com/">Nicklas Hansen</a></b> is a PhD candidate at UC San Diego, advised by Prof. Xiaolong Wang and Prof. Hao Su, and is interested in building scalable, robust, and open-source algorithms for decision-making.
    </p>
  </div>
</div>
<p><br /></p>


<div class="row" id="organizers">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>

<div class="row">

 <div class="col-xs-2">
    <a href="https://www.gujinwei.org/">
      <img class="people-pic" src="static/img/people/jinwei.jpg" />
    </a>
    <div class="people-name">
      <a href="https://www.gujinwei.org/">Jinwei Gu</a>
      <h6>NVIDIA</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://hengcv.github.io/">
      <img class="people-pic" src="static/img/people/heng.jpg" />
    </a>
    <div class="people-name">
      <a href="https://hengcv.github.io/">Heng Wang</a>
      <h6>NVIDIA</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://prithv1.xyz/">
      <img class="people-pic" src="static/img/people/prithvijit.jpeg" />
    </a>
    <div class="people-name">
      <a href="https://prithv1.xyz/">Prithvijit Chattopadhyay</a>
      <h6>NVIDIA</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://github.com/mingyuliutw">
      <img class="people-pic" src="static/img/people/mingyu.jpeg" />
    </a>
    <div class="people-name">
      <a href="https://github.com/mingyuliutw">Ming-Yu Liu</a>
      <h6>NVIDIA</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://www.humphreyshi.com/">
      <img class="people-pic" src="static/img/people/humphrey.jpg" />
    </a>
    <div class="people-name">
      <a href="https://www.humphreyshi.com/">Humphrey Shi</a>
      <h6>Georgia Tech</h6>
    </div>
  </div>

</div>
<!-- <p><br /></p>
<p><br /></p> -->
<!-- 
<div class="row" id="advisors">
  <div class="col-xs-12">
    <h2>Senior Advisors</h2>
  </div>
</div>

<div class="row">

  <div class="col-xs-2">
    <a href="http://optas.github.io">
      <img class="people-pic" src="static/img/people/achlioptas.jpg" />
    </a>
    <div class="people-name">
      <a href="http://optas.github.io">Panos Achlioptas</a>
      <h6>Steel Perlot</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://angelxuanchang.github.io/">
      <img class="people-pic" src="static/img/people/angel.jpg" />
    </a>
    <div class="people-name">
      <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>
      <h6>Simon Fraser University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://niessnerlab.org/members/matthias_niessner/profile.html">
      <img class="people-pic" src="static/img/people/matthias.jpg" />
    </a>
    <div class="people-name">
      <a href="https://niessnerlab.org/members/matthias_niessner/profile.html">Matthias Niessner</a>
      <h6>Technical University of Munich</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://geometry.stanford.edu/member/guibas/">
      <img class="people-pic" src="static/img/people/guibas.jpg" />
    </a>
    <div class="people-name">
      <a href="https://geometry.stanford.edu/member/guibas/">Leonidas J. Guibas</a>
      <h6>Stanford University</h6>
    </div>
  </div>
</div> -->

<p><br /></p>

<div class="row" id="contact">
  <div class="col-xs-12">
    <h2>Contact</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <!-- <p>
      To contact the organizers please use <b>worldmodelbench2@gmail.com</b>
    </p> -->
    <p>
      To contact the organizers please use <b>worldmodelbench2@gmail.com</b>
    </p>
  </div>
</div>
<p><br /></p>

<hr />

<div class="row">
  <div class="col-xs-12">
    <h2>Acknowledgments</h2>
  </div>
</div>
<p><a name="/acknowledgements"></a></p>
<div class="row">
  <div class="col-xs-12">
    <p>
      Thanks to <span style="color:#1a1aff;font-weight:400;"> <a href="https://languagefor3dscenes.github.io/">languagefor3dscenes</a></span> for the webpage format.
    </p>
  </div>
</div>

      </div>
    </div>

  </body>
</html>
